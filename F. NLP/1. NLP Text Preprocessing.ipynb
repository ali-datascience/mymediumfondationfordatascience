{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["_4x0SU7pavWs","o2L-YsJ9ayD8","_QyK_dvobO74","h6MB-AEVb7K0","4ph9tLyacMzY","gafWp2Mfcqfr","BcN8V-6Uc_JT","A7tUOmwadOuJ","pXTou8V0dcZ_","XbzFBQbDejz-","zlV7T8KJevmN","Gye5GBPDfJrE"],"toc_visible":true,"authorship_tag":"ABX9TyNG9h6Y49mmKo/IlpfabZQf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Text Preprocessing\n","    •Lowercase\n","    •Regular Expressions\n","    •Tokenization\n","    •Stop words removal\n","    •Stemming and Lemmatization\n","    •N-grams\n"],"metadata":{"id":"CqqpmKXZZ-DU"}},{"cell_type":"markdown","source":["## Lowercase"],"metadata":{"id":"_4x0SU7pavWs"}},{"cell_type":"code","source":["text = \"Natural Language Processing (NLP) is amazing! It's used in chatbots, search engines, and more.\"\n","\n","# Lowercasing\n","lowercase_text = text.lower()\n","print(\"Lowercase:\", lowercase_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDVtyvfzaHE_","executionInfo":{"status":"ok","timestamp":1733412164971,"user_tz":-420,"elapsed":971,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"84100b81-8136-45b6-b749-1daf6b8ba841"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Lowercase: natural language processing (nlp) is amazing! it's used in chatbots, search engines, and more.\n"]}]},{"cell_type":"markdown","source":["## Regex Operations"],"metadata":{"id":"o2L-YsJ9ayD8"}},{"cell_type":"markdown","source":["### Matching Patterns"],"metadata":{"id":"_QyK_dvobO74"}},{"cell_type":"code","source":["import re\n","\n","# example: matching a 4-digit number\n","pattern = r'\\d{4}'\n","text = 'The year is 2025'\n","\n","match = re.search(pattern, text)\n","if match:\n","    print(f\"Matched: {match.group()}\")\n","else:\n","    print(\"No match found\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24LaC3koalTI","executionInfo":{"status":"ok","timestamp":1733412367645,"user_tz":-420,"elapsed":388,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"74a770b1-d327-4abe-a750-09e2fa2ccce7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Matched: 2025\n"]}]},{"cell_type":"markdown","source":["### Extracting Substrings"],"metadata":{"id":"h6MB-AEVb7K0"}},{"cell_type":"code","source":["text = 'Contact us at info@example.com'\n","pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n","\n","emails = re.findall(pattern, text)\n","print(\"Extracted emails:\", emails)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ooMMfhiRbYwy","executionInfo":{"status":"ok","timestamp":1733412534137,"user_tz":-420,"elapsed":393,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"ff56d25a-caf0-4958-b8c0-8a5c98f62cda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracted emails: ['info@example.com']\n"]}]},{"cell_type":"markdown","source":["### Replacing Substrings"],"metadata":{"id":"4ph9tLyacMzY"}},{"cell_type":"code","source":["text = 'The upcoming movie in 12/30/2024 until 01-02-2025'\n","pattern = r'(\\d{2})[-/](\\d{2})[-/](\\d{4})'\n","replacement = r'\\3-\\1-\\2'  # reformat to YYYY-MM-DD\n","\n","formatted_text = re.sub(pattern, replacement, text)\n","print(\"Formatted text:\", formatted_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4KVyDpJcDOe","executionInfo":{"status":"ok","timestamp":1733412636096,"user_tz":-420,"elapsed":542,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"9d3d75e1-f0d3-484f-e5af-2a1d19e282bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Formatted text: The upcoming movie in 2024-12-30 until 2025-01-02\n"]}]},{"cell_type":"markdown","source":["### Splitting Strings"],"metadata":{"id":"gafWp2Mfcqfr"}},{"cell_type":"code","source":["# splitting a string by commas or spaces\n","text = 'apple, banana, cherry orange'\n","pattern = r'[,\\s]+'\n","\n","fruits = re.split(pattern, text)\n","print(\"Split text:\", fruits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Upvu4PAcq7-","executionInfo":{"status":"ok","timestamp":1733412752209,"user_tz":-420,"elapsed":363,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"122f9925-ff4d-4fe0-9b51-7389fead9838"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Split text: ['apple', 'banana', 'cherry', 'orange']\n"]}]},{"cell_type":"markdown","source":["### Removing Punctuation\n","Punctuation often doesn't contribute meaningfully to text analysis."],"metadata":{"id":"BcN8V-6Uc_JT"}},{"cell_type":"code","source":["import re\n","\n","text = \"Hello, World! NLP is amazing; isn't it?\"\n","clean_text = re.sub(r'[^\\w\\s]', '', text)  # Removes punctuation\n","print(\"Without punctuation:\", clean_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i5E3ppvXc4eQ","executionInfo":{"status":"ok","timestamp":1733412796442,"user_tz":-420,"elapsed":438,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"343e8b99-4b54-4cb6-fd89-47cc7bf6cd5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Without punctuation: Hello World NLP is amazing isnt it\n"]}]},{"cell_type":"markdown","source":["### Removing Numbers\n","For tasks where numbers are irrelevant, remove them."],"metadata":{"id":"A7tUOmwadOuJ"}},{"cell_type":"code","source":["text = \"The year is 2024, and I have 2 apples.\"\n","clean_text = re.sub(r'\\d+', '', text)  # Removes digits\n","print(\"Without numbers:\", clean_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sHnpP88udDQX","executionInfo":{"status":"ok","timestamp":1733412866179,"user_tz":-420,"elapsed":445,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"87f60f78-a849-4259-b589-cead9f46a278"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Without numbers: The year is , and I have  apples.\n"]}]},{"cell_type":"markdown","source":["### Lowercasing and Whitespace Normalization"],"metadata":{"id":"pXTou8V0dcZ_"}},{"cell_type":"code","source":["text = \"   NLP   is   FUN!  \"\n","clean_text = re.sub(r'\\s+', ' ', text.strip().lower())  # Normalize whitespace and lowercase\n","print(\"Normalized text:\", clean_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jcfp7zfBdUR3","executionInfo":{"status":"ok","timestamp":1733412914824,"user_tz":-420,"elapsed":423,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"e0981e11-ce4b-40cd-b70a-5a856df09b2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Normalized text: nlp is fun!\n"]}]},{"cell_type":"markdown","source":["### Extracting URLs"],"metadata":{"id":"XbzFBQbDejz-"}},{"cell_type":"code","source":["text = \"Visit https://example.com or http://another-site.org for more info.\"\n","urls = re.findall(r'https?://[^\\s]+', text)  # Matches HTTP and HTTPS URLs\n","print(\"Extracted URLs:\", urls)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WtOOJQbhdgH4","executionInfo":{"status":"ok","timestamp":1733413202975,"user_tz":-420,"elapsed":450,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"7b7d8b5d-c8c2-49ff-d9d9-6c94aa2eb0f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracted URLs: ['https://example.com', 'http://another-site.org']\n"]}]},{"cell_type":"markdown","source":["### Finding Hashtags and Mentions"],"metadata":{"id":"zlV7T8KJevmN"}},{"cell_type":"code","source":["text = \"Analyze #data and follow @data_science!\"\n","hashtags = re.findall(r'#\\w+', text)\n","mentions = re.findall(r'@\\w+', text)\n","print(\"Hashtags:\", hashtags)\n","print(\"Mentions:\", mentions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXf7GKAiemgW","executionInfo":{"status":"ok","timestamp":1733413297753,"user_tz":-420,"elapsed":376,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"ba64461d-1bf6-4d23-af0b-6c2be457cef7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hashtags: ['#data']\n","Mentions: ['@data_science']\n"]}]},{"cell_type":"markdown","source":["### Removing HTML Tags"],"metadata":{"id":"Gye5GBPDfJrE"}},{"cell_type":"code","source":["text = \"<html><body><p>Hello, World!</p></body></html>\"\n","clean_text = re.sub(r'<.*?>', '', text)\n","print(\"Without HTML tags:\", clean_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QREA0WLWe293","executionInfo":{"status":"ok","timestamp":1733413361181,"user_tz":-420,"elapsed":408,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"f1b48679-aea6-498c-9ea2-d048fd054d05"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Without HTML tags: Hello, World!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wSxpANdxfNIC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tokenization"],"metadata":{"id":"_GtZWypKflS4"}},{"cell_type":"code","source":["!pip install nltk --upgrade\n","import nltk\n","nltk.download('punkt_tab')\n","from nltk.tokenize import word_tokenize\n","\n","text = \"Hi, my name is Muhamad Ali from Data Science Methods\"\n","token = word_tokenize(text)\n","print(token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPd_lfacflp3","executionInfo":{"status":"ok","timestamp":1733415018172,"user_tz":-420,"elapsed":8556,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"e4facd7b-7875-4d6d-9111-17eed830caf9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["['Hi', ',', 'my', 'name', 'is', 'Muhamad', 'Ali', 'from', 'Data', 'Science', 'Methods']\n"]}]},{"cell_type":"markdown","source":["## Stop words removal"],"metadata":{"id":"YGqx0wBOkVeu"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","text = \"Hi, my name is Muhamad Ali. im a man who working as data scienctist\"\n","tokens = word_tokenize(text)\n","filtered_tokens = [word for word in tokens if not word in stop_words]\n","print(filtered_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YNiBYtOxkMJo","executionInfo":{"status":"ok","timestamp":1733415073300,"user_tz":-420,"elapsed":524,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"4a05ccc2-753e-4fdc-9350-6d5e8784ff0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hi', ',', 'name', 'Muhamad', 'Ali', '.', 'im', 'man', 'working', 'data', 'scienctist']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## Stemming and Lemmatization"],"metadata":{"id":"fghbVvk0lzUz"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer,WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt_tab')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"id":"eJ-cQobxlnh9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733455984158,"user_tz":-420,"elapsed":564,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"5ca15794-ef61-4679-9430-9ae622db0ddb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()"],"metadata":{"id":"pd4KWll3BlDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text='running runs ran'\n","tokens = word_tokenize(text)\n","stemmed_words = [stemmer.stem(word) for word in tokens]\n","lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n","print('Stemmed:',stemmed_words)\n","print('Lemmatized:',lemmatized_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0R-IZZOBpEk","executionInfo":{"status":"ok","timestamp":1733455989909,"user_tz":-420,"elapsed":4152,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"98128375-34e4-472e-a26d-25a49769cc9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Stemmed: ['run', 'run', 'ran']\n","Lemmatized: ['running', 'run', 'ran']\n"]}]},{"cell_type":"markdown","source":["## N-grams"],"metadata":{"id":"V8pq36tWBuWH"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","text = [\"I love programming in Python\"]\n","\n","# Initialize CountVectorizer with n-gram range\n","vectorizer = CountVectorizer(ngram_range=(2, 2))  # bigrams\n","ngrams = vectorizer.fit_transform(text)\n","\n","# Get feature names (bigrams)\n","print(\"Bigrams:\", vectorizer.get_feature_names_out())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gGy6gXovBrF2","executionInfo":{"status":"ok","timestamp":1733456424097,"user_tz":-420,"elapsed":571,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"ff135085-858e-4d9e-c14e-3ab11af1014c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bigrams: ['in python' 'love programming' 'programming in']\n"]}]},{"cell_type":"code","source":["from nltk.util import ngrams\n","\n","text = \"I love programming in Python\"\n","words = text.split()\n","\n","# Generate 2-grams (bigrams)\n","bigrams = list(ngrams(words, 2))\n","print(\"Bigrams:\", bigrams)\n","\n","# Generate 3-grams (trigrams)\n","trigrams = list(ngrams(words, 3))\n","print(\"Trigrams:\", trigrams)\n","\n","from nltk.util import ngram\n","\n","bigram = list(ngram(words,2))\n","trigram = list(nga)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttBtH3vqDefr","executionInfo":{"status":"ok","timestamp":1733456434130,"user_tz":-420,"elapsed":568,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"8d3306c0-55fc-4872-a30a-5116219eefbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bigrams: [('I', 'love'), ('love', 'programming'), ('programming', 'in'), ('in', 'Python')]\n","Trigrams: [('I', 'love', 'programming'), ('love', 'programming', 'in'), ('programming', 'in', 'Python')]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"jrWaGK7uDg8k"},"execution_count":null,"outputs":[]}]}