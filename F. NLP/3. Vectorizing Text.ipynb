{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYpcWRhi4RdlN4/YPsgaka"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Bag of Words Model"],"metadata":{"id":"p5P2PXANkaeq"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Sample text\n","texts = [\"small dog\", \"cute cat\", \"cute dog\"]\n","\n","# Convert to Bag of Words\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(texts)\n","\n","# Vocabulary and representation\n","print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n","print(\"Bag of Words Representation:\\n\", X.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HkFwBcq9kb9B","executionInfo":{"status":"ok","timestamp":1734573916547,"user_tz":-420,"elapsed":332,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"fe099506-1d85-42ee-efea-99e0a56e3c7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: ['cat' 'cute' 'dog' 'small']\n","Bag of Words Representation:\n"," [[0 0 1 1]\n"," [1 1 0 0]\n"," [0 1 1 0]]\n"]}]},{"cell_type":"markdown","source":["# TF-IDF"],"metadata":{"id":"zK4DJGOhkdP7"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Convert to TF-IDF\n","tfidf_vectorizer = TfidfVectorizer()\n","X_tfidf = tfidf_vectorizer.fit_transform(texts)\n","\n","# view the TF-IDF values for the first document\n","feature_names = tfidf_vectorizer.get_feature_names_out()\n","first_document_vector = X_tfidf[0]\n","\n","print(\"Feature names:\", feature_names)\n","print(\"TF-IDF values for the first document:\")\n","print(first_document_vector.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"do7NhnHlkeHj","executionInfo":{"status":"ok","timestamp":1734573919884,"user_tz":-420,"elapsed":620,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"831789f8-322c-48a0-98e9-4323f5fda265"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature names: ['cat' 'cute' 'dog' 'small']\n","TF-IDF values for the first document:\n","[[0.         0.         0.60534851 0.79596054]]\n"]}]},{"cell_type":"markdown","source":["# Word Embeding"],"metadata":{"id":"2PvQakbWvq5J"}},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","\n","# Example sentences\n","sentences = [[\"I\", \"love\", \"NLP\"], [\"NLP\", \"is\", \"awesome\"]]\n","\n","# Train Word2Vec model\n","model = Word2Vec(\n","    sentences=sentences,    # Input data\n","    vector_size=10,         # Dimension of word embeddings\n","    window=3,               # Context window size\n","    min_count=1,            # Minimum word frequency\n","    workers=4,              # Number of CPU threads\n","    sg=1                    # Use Skip-Gram (1) or CBOW (0)\n",")\n","\n","# Get vector for a word\n","print(\"Vector for 'NLP':\", model.wv['NLP'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RsrYk5hmvr6J","executionInfo":{"status":"ok","timestamp":1733468233003,"user_tz":-420,"elapsed":335,"user":{"displayName":"Data Science Methods","userId":"14154996411222883600"}},"outputId":"3730d59f-8a47-4e72-dab8-9ea6692c7a9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vector for 'NLP': [-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809\n","  0.06458873  0.08972988 -0.05015428 -0.03763372]\n"]}]},{"cell_type":"markdown","source":["# Comparations"],"metadata":{"id":"Pcn7BrVF0jSp"}},{"cell_type":"markdown","source":["\n","# Comparison: CountVectorizer, TF-IDF, and Word2Vec\n","\n","| **Feature**           | **CountVectorizer**                                       | **TF-IDF**                                            | **Word2Vec**                                         |\n","|-----------------------|-----------------------------------------------------------|------------------------------------------------------|-----------------------------------------------------|\n","| **What It Does**      | Converts text into a matrix of word counts.               | Converts text into a matrix with term frequencies adjusted by their inverse document frequency. | Converts words into dense vectors that capture semantic meaning. |\n","| **Representation**    | Sparse matrix of word counts.                            | Sparse matrix, but adjusts word frequencies based on their importance. | Dense vector representations of words. |\n","| **Contextual Awareness**| No context; based only on word occurrence.               | No context; focuses on word importance in documents.   | Captures semantic relationships and context.          |\n","| **When to Use**       | When you need simple frequency-based features.            | When you want to weigh words by their relevance to the document and corpus. | When you need to capture word meanings and relationships. |\n","| **Best for**          | Text classification, keyword extraction, basic NLP tasks.  | Document classification, keyword extraction, search engines. | Sentiment analysis, language modeling, machine translation, word similarity. |\n","| **Complexity**        | Simple, easy to understand and implement.                 | Slightly more complex due to inverse document frequency (IDF). | More complex; requires large datasets or pre-trained embeddings. |\n","| **Example NLP Tasks** | - Document classification (e.g., spam vs. non-spam)      | - Information retrieval (e.g., search engines)       | - Sentiment analysis, text generation, word similarity |\n","| **When to Use**       | - Small to medium datasets.                              | - When importance of words needs to be weighed.        | - Large datasets where semantic meaning is important. |\n","| **Advantages**        | - Simple and interpretable.                              | - Helps with document-level importance of terms.      | - Encodes semantic meaning and relationships.          |\n","| **Disadvantages**     | - Ignores context; leads to sparse matrices.              | - Ignores word order and context; still sparse.        | - Requires large data to train effectively; harder to interpret. |\n","\n","---\n","\n","### Example Use Cases\n","\n","| **Task**                             | **CountVectorizer**                                   | **TF-IDF**                                          | **Word2Vec**                                        |\n","|--------------------------------------|-------------------------------------------------------|----------------------------------------------------|----------------------------------------------------|\n","| **Text Classification (e.g., spam detection)** | Use word counts to classify documents as spam or not.   | Use weighted word counts for better classification, emphasizing rare terms. | Use embeddings to classify text based on semantic meaning. |\n","| **Keyword Extraction**               | Identify frequently occurring words.                  | Extract important keywords based on document relevance. | Identify similar words based on vector proximity. |\n","| **Sentiment Analysis**               | Not ideal due to lack of context.                     | Can be used for sentiment analysis by weighing important words. | Best for sentiment analysis by capturing word meanings in context. |\n","| **Document Retrieval**               | Simple search based on word frequency.                | Improve search by using word importance (TF-IDF).   | Semantic search based on word meanings. |\n","| **Clustering Text Data**             | Group similar documents based on word counts.         | Group documents based on significant terms with TF-IDF scores. | Group documents by similarity in semantic space (using Word2Vec embeddings). |"],"metadata":{"id":"pIhVLc9E0pSM"}},{"cell_type":"code","source":[],"metadata":{"id":"G1qqc1PRwTt2"},"execution_count":null,"outputs":[]}]}